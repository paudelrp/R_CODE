---
title: 'Midterm2'
author: "Rabin Paudel"
date: "11/15/2021"
output: html_document
---

### Problem 1 (30 pts)
#### You can extract the raw data for this table as follows.


```{r}
library("stringr")
library("tidyverse")
#install.packages("dplyr")
library(dplyr)
library(ggplot2)
library("pdftools") ## install.library("pdftools") if this library is missing
temp_file <- tempfile()
url <- paste0("https://www.pnas.org/content/suppl/2015/09/16/",
"1510159112.DCSupplemental/pnas.201510159SI.pdf")
download.file(url, temp_file)
txt <- pdf_text(temp_file)
file.remove(temp_file)
## [1] TRUE
raw_data <- txt[2]
raw_data
```

#### First, use the function str_split from the stringr library to split the 
#### raw_data string into lines. For example.

```{r}
raw_data_lines <- stringr::str_split(raw_data, "\n")[[1]]
head(raw_data_lines, 8)
```

#### The numbers in the tables are recorded in the 7th through 15th 
#### lines/elements of raw_data_lines.

```{r}
tab_data <- raw_data_lines[7:15]
tab_data
```

#### We can now try to use str_trim and str_split to split each line into 
#### separate columns. We might want to take a careful look at the arguments 
#### of str_split or try out the examples provided in the help page for
#### str_split.Then,After doing the above steps, you should now have something 
#### resembling what we want. Now add the column names and remember to convert 
#### the values in most of the columns into numbers. The function across in 
#### dplyr might be useful here.
#### As always, refer to do this cheatsheet.


### This is the required table.
```{r}
tab_data1 <- str_trim(tab_data, side = c("both"))
tab_data2 <- str_split(tab_data1, "\\s{2,}", simplify = TRUE) %>% 
  data.frame() %>%
  setNames(c('discipline', 'app_T', 'app_M', 'app_F', 'awards_T', 'awards_M',
             'awards_F', 'success_rates_T', 'success_rates_M', 
             'success_rates_F')) %>% mutate_at(-1, parse_number) %>% 
  as_tibble()
tab_data2
```

### Problem 2 (30pts)

#### The data had been studied in this article and is included as part the 
#### SemiPar library in R.

```{r}
library(SemiPar) ## install.packages("SemiPar") if this library is missing
data(milan.mort)
head(milan.mort)

```

#### Try to split day.num variable into month to pridect resp.mort
```{r}
milan.mort<- milan.mort %>% 
  mutate(month = trunc((day.num/30 )%%12))
head(milan.mort)
```



#### Split the data into a random training and testing chunk.

```{r}
train_idx <- sample(1:nrow(milan.mort), 0.8*nrow(milan.mort), replace = FALSE)
milan_mort_train <- milan.mort[train_idx,]
## Training data contains 80% of the observations
milan_mort_test <- milan.mort[-train_idx,]
## Testing data contains 20% of the observations
names(milan_mort_train)
```

#### Try to change int variable into the factor and character variable like 
#### day.of.week and holiday.

```{r}
milan_mort_train$holiday <- as.factor(milan_mort_train$holiday)
milan_mort_train$day.of.week <- as.factor(milan_mort_train$day.of.week)
milan_mort_train$month <- as.character(milan_mort_train$month)
str(milan_mort_train)
```

#### Fit model on training data

```{r}
mod_naive <- lm(resp.mort ~ SO2 + TSP, milan_mort_train)

```
#### Predicted value on test data

```{r}
milan_mort_test_predict <- predict(mod_naive, milan_mort_test)

```
#### Mean absolute error of prediction

```{r}
mae_naive <- mean(abs(milan_mort_test_predict - milan_mort_test$resp.mort))
mae_naive

```

#### Your completed answer should have the following components.
### You should consider two or three different models.
#### The first model is a very simple/simplistic model that serves as a naive 
#### baseline. Try to look relation between the variable.

```{r}
#pairs.panels(milan_mort_train)
```
#### We know resp.mort and tot.mort are highly correlated, and SO2 and TSP also 
#### correlated. The variable resp.mort is right skew. 

### First Model

```{r}
mod0 <- lm(resp.mort ~ SO2 + TSP, milan_mort_train)
mod0
summary(mod0)
AIC(mod0)

milan_mort_test_predict <- predict(mod0, milan_mort_test)
## Mean absolute error of prediction
mae_naive <- mean(abs(milan_mort_test_predict - milan_mort_test$resp.mort))
mae_naive
```
### Second Model
#### The second model is a suï¬€iciently complicated model (but should not have 
#### say, more than 15 coeï¬€i-cients).

```{r}
mod1 <- lm(resp.mort ~SO2 + TSP + rel.humid + mean.temp, milan_mort_train)
mod1
AIC(mod1)
summary(mod1)
AIC(mod1)
```
### Third Model
```{r}
#Try to remove non significance variable 
mod2 <- lm(resp.mort ~.-TSP, milan_mort_train)
mod2
summary(mod2)

library(forcats)
# Try to remove non significance variable 
milan_mort_train %>% select(day.of.week) %>% pull() %>% table
milan_mort_train %>% select(month) %>% pull() %>% table
milan_mort_train1 <- milan_mort_train%>% mutate(day.of.week = 
                                       fct_collapse(day.of.week,                                                      "3" = c("5", "6", "7")))
milan_mort_train2 <- milan_mort_train1 %>% mutate(month = 
                        fct_collapse(month, 
                            "0" = c("4","5", "6", "7", "8", "9", "10", "11")))
head(milan_mort_train2)

mod2 <- lm(resp.mort ~.-TSP-holiday-mean.temp-tot.mort - rel.humid,
           milan_mort_train2)
mod2
summary(mod2)

AIC(mod2)

```
#### After removing non significant value from the model.

```{r}
#Try to remove non significance variable 
mod2 <- lm(resp.mort ~.-TSP, milan_mort_train)
mod2
summary(mod2)

library(forcats)
# Try to remove non significance variable 
#milan_mort_train %>% select(day.of.week) %>% pull() %>% table
milan_mort_train %>% select(month) %>% pull() %>% table
#milan_mort_train1 <- milan_mort_train%>% mutate(day.of.week = 
#                                       fct_collapse(day.of.week,                                                   # # "3" = c("5", "6", "7")))
milan_mort_train2 <- milan_mort_train %>% mutate(month = 
                        fct_collapse(month, 
                            "0" = c("4","5", "6", "7", "8", "9", "10", "11")))
head(milan_mort_train2)

mod2 <- lm(resp.mort ~.-TSP-holiday-mean.temp-tot.mort -day.of.week- rel.humid,
           milan_mort_train2)
mod2
summary(mod2)

AIC(mod2)

```
```{r}
## Average number of deaths per day using the training data
estimate <- mean(milan_mort_train2$resp.mort)
estimate

value <- mean(abs(estimate - milan_mort_test$resp.mort))
value
```

### (2) You should provide some brief discussion on.
### (a) why you include these predictor variables in your model
#### ANS: My best model is model 3 and my predicted variable day.num and three
#### month period Jan, Feb, march, and SO2 gas these variable are highely sig-
#### nificant. The model three also AIC and residual standard error are small
#### than other two model, so model three is best fit. Overall F test and p-value 
#### both highly significant. My conclusion is that in highly SO2 gas in air 
#### in the cold season are more problematic more people are died for 
#### respiratory disease.

### (b) how satisfied you are with the accuracy of your model and.

#### ANS: I think my model is good enough through the given data set.
#### I have no idea why R^2 and AdjR^2 are so small. 

### (c) if appropriate) are there any issues with your model and what will you 
#### try/do if you have more time (for example, there could be seasonal trend,
#### the response are integers not real numbers, ...)
#### ANS: Yes, I converted some variable into the factor as well as char variable.
#### I convert day number as a month variable to figure out weather is important
#### or not,but I found weather is important cold season more problem of 
#### respiratory related disease.I try to research day.of.week variable, 
#### but these variable not significant.

## Problem 3(15pts)

#### The data is available here and can be read into R as follows

```{r}
df <- readr::read_table("http://stat.rutgers.edu/home/mxie/stat586/homework/hw2_junka_data.txt")
df
```

#### For this problem, see the lecture slides on penalized splines. In particular the following two functions

```{r}
basis.design <- function(x,knots){
Xmat <- cbind(rep(1, length(x)), x)
for(i in 1:length(knots)){
Xmat <- cbind(Xmat, ifelse(x < knots[i], 0, x - knots[i]))
}
return( Xmat)
}
psr <- function(y,x,knots,lambda){
X <- basis.design(x, knots)
D <- diag(c(0,0, rep(1,length(knots))))
S <- X %*% solve(t(X) %*% X + lambda*D) %*% t(X)
yhat <- S %*% y
df_fit <- sum(diag(S)) ## degrees of freedom for the fit
n <- nrow(X)
gcv <- sum((y - yhat)^2)/(1 - df_fit/n)^2 ## Generalized cross-validation score
return(list(yhat = yhat, gcv = gcv)) ## Return the fitted value and the gcv score
}
```

#### The (solid) regression line and (dashed) curve in the above plot correspond
#### to a simple least square regression line log(hardnessð‘–) â‰ˆ ð›½0 + ð›½1density
####ð‘– and a nonparametric regression line, respectively. The nonparametric
#### regression line is fitted via penalized spline regression of the form

```{r}
knots <- seq(from = 0, to = 70, by = 10) # use the function.
val <- psr(log(df$Hard), df$Dens, knots,1) # create val.
val
newData <- data.frame(df,val) # Create new data frame.
names(newData)
newData

# Try to create the model.
mu_x <- mean(newData$Dens)
mu_y <- mean(log(newData$Hard))
s_x <- sd(newData$Dens)
s_x
s_y <- sd(log(newData$Hard))
r <- cor(newData$Dens,log(newData$Hard))
r
fit <- lm(log(newData$Hard) ~ Dens + yhat , data = newData) # Fit the line.
summary(fit)

df <- newData %>% mutate(rp = predict(fit))
df
#Using the ggplot and plot the data. 
#This is required chart.
ggplot(df, aes(Dens, log(Hard))) + geom_point() +
  geom_line(aes(Dens, rp), color = 'blue', linetype = 'dashed') + 
  geom_abline(slope = r * s_y/s_x, intercept = mu_y - r * s_y/s_x * mu_x,color = 'blue') 

```

## THE END
## THANK YOU!