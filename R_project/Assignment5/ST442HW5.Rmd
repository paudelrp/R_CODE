---
title: "Assignment5"
author: "Rabin Paudel"
date: "11/1/2021"
output: pdf_document
---

## Problem 1 (20pts)
#### A snippet of the data is given below

```{r}
## install.packages("alr4")
library(ggplot2)
library(dplyr)
library(alr4)
data(MWwords)
```

```{r}
newData <- select(MWwords, Hamilton, HamiltonRank) %>% head(50)
```
### 1.(a) sing only the 50 most frequent words in Hamilton’s work, fit a simple 
### linear regression model with log(Hamilton) as the response variable.

#### We fit the simple linear regression model with log(Hamilton) as the 
#### response variable.

```{r}
model <- lm(log(newData$Hamilton) ~ newData$HamiltonRank, data = MWwords)
summary(model)
fg <- ggplot(newData, aes(HamiltonRank, log(Hamilton))) 
fg + geom_point() + geom_smooth(method = "lm", se = FALSE)
```

### 1.(b) is the empirical law γ ≈1 likely to be “correct’ ’ ? Justify 
#### answer.

#### Both log of the rank and log of frequency both closer to the intercept
#### so that the the empirical law γ ≈1 likely to be "correct".

```{r}
modele <- lm(log(newData$Hamilton) ~ log(newData$HamiltonRank), data = MWwords)
modele
summary(modele)
```

### 1.(c) Repeat part (a) and (b), first for the 75 most frequent words in 
### Hamilton’s work and then for the 100 most frequent words in Hamilton work’s. 
### Is the relationship posited by Zipf still reasonable in these cases ?

```{r}
new1Data <- select(MWwords, Hamilton, HamiltonRank) %>% head(75)
model1 <- lm(log(new1Data$Hamilton) ~ new1Data$HamiltonRank, data = MWwords)
summary(model1)
fg1 <- ggplot(new1Data, aes(HamiltonRank, log(Hamilton))) 
fg1 + geom_point() + geom_smooth(method = "lm", se = FALSE)

```

```{r}
new2Data <- select(MWwords, Hamilton, HamiltonRank) %>% head(100)
model2 <- lm(log(new2Data$Hamilton) ~ new2Data$HamiltonRank, data = MWwords)
summary(model1)
fg1 <- ggplot(new2Data, aes(HamiltonRank, log(Hamilton))) 
fg1 + geom_point() + geom_smooth(method = "lm", se = FALSE)
```

#### Yes, the relationship posited by Zipf still responable in these case 
#### Residual standard error small, adj R sqare also big, median value is small, 
#### bothcase, p-value is small that way highly significant. 

### 1.(d) Now investigate Zipf law for the Simpson Frequency Dictionary dataset. 
### You will need to do some preprocessing and possibly cleaning of the raw data
### (in particular the numbers in the Frequency column 1 of this dataset; 
### your stringr skills might come in handy here

```{r}
library(stringr)
library(dplyr)
library(tidyr)
READ <- read.csv("https://pastebin.com/raw/anKcMdvk", header = FALSE, 
                skip = 5, stringsAsFactors = TRUE)
READ %>% head(30)
```

#### We need to clean the data set, so I am going to clean the data Trying to
#### first 500 values. 

```{r}
Rank <- as.numeric(str_sub(READ$V1, 1, 7), replace = TRUE)
data.frame(Rank) %>% head(10)

val <- data.frame(Rank)

Word <- str_sub(READ$V1, 8, 25)
Word %>% head(10)

val1 <- data.frame(Word)

Frequency <- as.numeric(str_sub(READ$V1, 40, -2), replace = TRUE)
Frequency %>% head(10)

val2 <- data.frame(Frequency) 

ADDDATA <- data.frame(c(val, val1, val2))
ADDDATA %>% head(10)

jk <- ADDDATA %>% head(500)

fg3 <- ggplot(jk, aes(log(Rank), log(Frequency))) 

fg3 + geom_point() + geom_smooth(method = "lm", se = FALSE) 
modele1 <- lm(log(Frequency) ~ Rank, data = jk)
summary(modele1)

```
#### Adj R square is good enough, p- value small enough data is significant, Me-
#### dian value close to jeso, residual standard error = 0.379 (small) so that 
#### the data look like normal. So, the data follows Zipf law. 

## Problem 2 (20 pts)

### This problem uses the following Kelley Blue Book dataset on the selling 
### price of used GM cars. The data is available online here. You can read in 
### the data using the following code chunk.

```{r}
kbb <- read.csv("https://bit.ly/3GknAkw", header = T, sep = ",")
kbb %>% head(10)
```

### Using this data, what is the “best” (linear) model you can find for 
### predicting the sell price ? Note that this is an open-ended problem.

#### We now simple model predicting sell price to get moodle only with Mileage

```{r}
mod0 <- lm(Price~ Mileage, data = kbb)
summary(mod0)
```
#### To find mean absolute error of mod0
```{r}
mae_mod0 <- mean(abs(kbb$Price - mod0$fitted.values)) 
mae_mod0
```

#### This value small Adj R^2 not good enough.
#### Residual Standard error = 9789 (Big)
#### P-value small highly significant 
#### Higher median value (not good enough)
#### F and t statistic good enough
#### Negative correlation Price and Milege (sloop of milage is negative)
#### ## Mean Absolute Error = 7596.28 (high)

#### Next model mod1 add variable Cylinder + Liter + Doors + some dummy Variable.

```{r}
mod1 <- update(mod0, .~. + Cylinder + Liter + Doors + Cruise + Sound + Leather)
summary(mod1)
```

#### To find mean absolute error of mod0
```{r}
mae_mod1 <- mean(abs(kbb$Price - mod1$fitted.values))
mae_mod1
```

#### The variable Liter is not significant higher p-value so i am going to 
#### Remove the variable.

```{r}
mod1 <- update(mod0, .~. + Cylinder + Doors + Cruise + Sound + Leather)
summary(mod1)
```
#### We don't see any big change after removing the variable Liter.

#### To create another model 

```{r}
mod2 <- update(mod1, .~. + Make + Model + Trim + Type + Cruise + Sound + Leather)
summary(mod2)
```
#### To find the absolute error of mod2

```{r}
mae_mod2 <- mean(abs(kbb$Price - mod2$fitted.values))
mae_mod2
```
####  Type variabile is remove cause the values of type all NA.

```{r}
mod2 <- update(mod1, .~. + Make + Model + Trim -Type + Cruise + Sound + Leather)
summary(mod2)
```

#### Median is more small then before.
#### Adj R^2 99.16 less (good enough).
#### All must values are significant.
#### Residual Standard error 903 doller and Mean absolute error 613 (close)

```{r}
kbb %>% filter(Model == "Vibe" | Trim == "SLE Sedan 4D" ) 

library(forcats)

kbb %>% select(Model, Trim) %>% pull() %>% table

kbb <- kbb %>% 
  mutate(Model = forcats::fct_collapse(Model, 
                    Vibo = c("AVEO", "Ion", "Monte Carlo", "Park Avenue"))) %>% 
  mutate(Trim = fct_collapse(Trim, Bal = c("SLE Sedan 4D", "SLE Sedan 4D",
                               "SS Coupe 2D", "SVM Sedan 4D",
                                "LT Coupe 2D",
                                  "SE Sedan 4D",                                                                                    
                              "SVM Hatchback 4D")))


mod2 <- update(mod2, data = kbb)
summary(mod2)

plot(mod2, sub.caption = " ", which = c(1,3,5))
```

#### Nor q-q plot little right skew, all most look like normal.
#### root standardized residuals vs fitted values find some outloiers overall 
#### not bad.
#### Find some outlier standardized residuals Vs Leverage.

```{r}
mod2_log_SP <- update(mod2, log(Price) ~ .)
summary(mod2_log_SP)$sigma

mae_mod2_log_SP <- mean(abs(log(kbb$Price) - mod2_log_SP$fitted.values))
mae_mod2_log_SP

```
#### Estimate relative error 1.92% to 2.61% Not Bad 
#### Over analysis shows that best fit model is mod2

### Writeup a short discussion summarizing the three main choices you made in 
### choosing a suitable model for this problem as well as what you learn when 
### trying to find a suitable model.

#### The three main choice are: 1. Try to make residual standard error and mean 
#### absolute error are small.2. Try to make Adj R^2 and R^2 value big and 
#### p-value highly significant. We do remove not significant value on the model
#### 3. We plot diffrent plot for example q-q plot, root standardized residuals
#### vs fitted values. And, plot standardized residuals Vs Leverage.
#### These are important point to figure out good model. 

### The “best” model I was able to find has a residual standard error of roughly 
### 520$ but my model could very much be overfitting to the data.

#### I am trying to do small residual standard error roughly I get around 900$
#### I know that is still high. 

## THE END

