---
title: "ST442 Project"
author: "Rabin Paudel"
date: "12/01/2021"
output: pdf_document
---

# _Abstract_
#### The name of data set of this project is _Heart Failure Prediction dataset_, and sorce is kiggle.com. In this data set, there are twelve prediction variable to predict death events. According to the description of data, "cardiovascular diseases (CVDs) are the number one cause of death globally taking an estimate 17.9 million lives each year which accounts for 31% of all deaths worldwise". First, I am going to analyze Multiple logistic regression to find the best model to predict death event cause by heart failure. In the model selection process, I am going to utilize forward selection, backward elimination, and stepwise selection process it terms of non significant (higher p-value) first eliminate. For my first pririty minimum value of AIC(Akaike information Criterion) and BIC(Bayesian Information Criterion). The Formula of AIC = -2(lnL) + 2k and BIC = -2(lnL) + 2k.In this project, I am going to present graphical presentation with diffrent way. We considered using a linear regression model to represent these probabilities: $\Large p(Y_{i})= \beta_{0}+\beta_{i}X_{i}$. In logistic regression, we use the logistic function, 
$$log(\frac{p(X_{i})}{1-p(X_{i})})= \beta_{0}+\beta_{i}X_{i}$$

#### The ligistic regression gives the assumption of the form of probability of failure to allow us to predict a plausible probability. 
#### probability of failure simple ligistic regression or we can add multiple case more variable 
$$p[failure] = [\frac{exp(\beta_{0}+\beta_{1}X_{i})}{(1+exp(\beta_{0}+\beta_{1}X_{i})}]$$

#### In summary, logistic regression can be thought of as either. 
#### (a) Empirical risk minimization where we replace 0-1 loss $I(g(X_{i}\neq Y_{i}))$ with logistic $log(1 + exp(-Y_{i}f(X_{i})))$ 
#### (b) Classification where we model $\eta(x) = P[Y_{i}| X=x] = exp(\frac{f(x)}{1+exp(f(x))})$
#### (c) maximum likelihood estimation for a collection of independent Bernoulli random variables, $Y_{i}$ with $pr[Y_{i} = 1] = \frac{exp(f(X_{i}))}{(1+exp(f(X_{i})))}$
# _Data Analysis: Read Data and Visual Representation of data_ 
#### Read the Data with csv file from the Desktop (download in Desktop).

```{r}
data <- read.csv("~/Desktop/heart_failure_clinical_records_dataset.csv")
# To Know the variable and type 
names(data)
str(data)
```

#### Column Variable name: 
#### Age:
#### anaemia: Decrease of red blood cells or hemoglobin (boolean)
#### creatinine_phosphokinase: Level of the CPK enzyme in the blood (mcg/L)
#### diabetes: if the patient has diabetes (boolean)
#### ejection_fraction: Percentage of blood leaving the heart at each contraction (percentage)
#### high_blood_pressure: If the patient has hypertension(boolean)
#### platelets: Platelets in the blood (kiloplatelets/mL)
#### serum_creatinine: Level of serum creatinine in the blood (mg/dL)
#### serum_sodium: Level of serum sodium in the blood(mEq/L)
#### sex: Woman or man (binary)
#### smoking: If the patient smokes or not (boolean)
#### time: Follow-up perriod (days)
#### DEATH_EVENT: If the patient deceased during the follow-up period (boolean)

#### Refine or modified the Data Changing the name and variable type.

```{r}
library(dplyr)
data[data$sex == 0,]$sex <- "Female"
data[data$sex == 1,]$sex <- "Male"
data[data$anaemia == 0,]$anaemia <- "No"
data[data$anaemia == 1,]$anaemia <- "Yes"
data[data$high_blood_pressure == 1,]$high_blood_pressure <- "Yes"
data[data$high_blood_pressure == 0,]$high_blood_pressure <- "No"
data[data$diabetes == 0,]$diabetes <- "No"
data[data$diabetes == 1,]$diabetes <- "Yes"
data[data$smoking == 0,]$smoking <- "No"
data[data$smoking == 1,]$smoking <- "Yes"
data$DEATH_EVENT <- ifelse(test = data$DEATH_EVENT == 0, yes = "Survived", no = "Dead")
data$DEATH_EVENT <- as.factor(data$DEATH_EVENT)

```

#### Creating graph to compression between the factor variable with DEATH_EVENT or like a visual representation of the data. Those bar diagram (visualization figure) clearly shows that not smoking, not high blood pressure, not diabetes, and less anaemia higher survival rate. 


```{r}
library(ggplot2)
library(tidyverse)
data1 <- data.frame(data) %>%
  select (anaemia,diabetes,high_blood_pressure, sex, smoking,DEATH_EVENT) %>%
  gather(key = "key", value = "value", -DEATH_EVENT)

```

```{r}
  
  #Visualize with bar plot
data1 %>% 
  ggplot(aes(value)) +
  geom_bar(aes(x        = value, 
               fill     = DEATH_EVENT), 
           alpha    = .6, 
           position = "dodge", 
           color    = "black",
           width    = .8
  ) + 
  labs(x = "",
       y = "",
       title = "Scaled Effect of Categorical Variables") +
  theme(
    axis.text.y  = element_blank(),
    axis.ticks.y = element_blank()) +
  facet_wrap(~ key, scales = "free", nrow = 3) +
  
scale_fill_manual(
          values = c("#0000FF", "#FF00FF"),
         name   = "Heart\nDisease",
         labels = c("Survived", "Dead"))
      
```

```{r}
#Visualize with box plot
data2 <- data %>% select(age, creatinine_phosphokinase, ejection_fraction, platelets, serum_creatinine, serum_sodium, time, DEATH_EVENT) %>% gather(key   = "key", 
         value = "value",
         -DEATH_EVENT)

data2 %>% 
  ggplot(aes(y = value)) +
  geom_boxplot(aes(fill = DEATH_EVENT), 
           alpha    = .6, 
           fatten   = 0.7
  ) + 
  labs(x = "",
       y = "",
       title = "Boxplotes for Numeric Variables") +
  theme(
    axis.text.y  = element_blank(),
    axis.ticks.y = element_blank()) +
  facet_wrap(~ key, scales = "free", nrow = 3) +
  
scale_fill_manual(
          values = c("#0000FF", "#FF00FF"),
         name   = "Heart\nDisease",
         labels = c("Survived", "Dead"))
      
```

#### Those box plot diagram (visualization figure) clearly shows that less age, higher ejection_fraction, higher serum_sodium, and more time follow up are higher survival rate. 

#### Highly correlated variables can lead to overly complicated models. So, ggcorr() function from GGally package provides a nice, clean correlation matrix of the numeracic variable. 

```{r}
library(GGally)
data %>% ggcorr(high = "gray20",
                low = "blue4",
                label      = TRUE, 
                hjust      = .75, 
                size       = 3, 
                label_size = 3,
                nbreaks    = 5
                ) +
  labs(title = "Correlation Matrix",
  subtitle = "Pearson Method Using Pairwise Obervations")

```

# _Way of Model Bulding:_
#### I like to describe the way of model-building multiple Logistic regression such as **Forward Selection**, **Backward Elimation**, and **Stepwise Selection Sequence**. **Forward Selection**: Inter the variables in order of terms with highest score statistic. **Backward Elimination**: In this process, starts with all terms in the model and droups them out in order according to the smallest wald statistic. **Stepwise**: Just like forward selection except that variables can be deleted from the model if p-values are above slstay. I am going to use **Model-Producing Methods** such as:* Akaike Information Criterion** AIC = -2(lnL) + 2k Where k = number of terms in the model (including intercept) ** Bayesian Information Criterion** BIC = -2(lnL)+(ln n)k.So, we want AIC and BIC is smaller.

### Fit multiple logistic regression

```{r}
fit1 <- glm(DEATH_EVENT ~ age + anaemia + creatinine_phosphokinase + diabetes + ejection_fraction + high_blood_pressure + platelets +  serum_creatinine + serum_sodium + sex + smoking + time, family = binomial(link = "logit"), data = data)
summary(fit1)
BIC(fit1)
```

#### The process backward elimination, stepwise selection sequence, and forward process I found the best multiple ligistic regression model is: 
```{r}
fit2 <- glm(DEATH_EVENT ~ age + ejection_fraction + serum_sodium + time, family = binomial(link = "logit"), data = data)
summary(fit2)
BIC(fit2)
```


### The value age, ejection_fraction, serum_sodium, and time are highly significant and AIC and BIC are also small. So, I found Best Model to predict is: 
### DEATH_EVENT = 12.498 + 0.045 * age - 0.068 * ejection_fraction - 0.082 * serum_sodium - 0.020 * time. 
#### I try to find to predict prob. of DEATH_EVENT vs time with ligistic regression to look the data with ligistic curve. 
```{r}
library("faraway")
df <- data %>% mutate(x2 = ifelse(data$DEATH_EVENT=="Dead",1,0))
plot(x = df$time, y = df$x2, ylim = c(0, 1),xlim = c(0,150), xlab = "Time",
ylab = "Prob. of DEATH_EVENT")
x <- seq(0, 150, 1)
logitmod <- glm(cbind(df$x2) ~ time, family = binomial, data = df)
lines(x, ilogit(coef(logitmod)[1] + coef(logitmod)[2] * x), col = 2)

```

#### Some theorical expression of Classification and regression trees suppose we are given ${(X_{i},Y_{i})}_{i=1}^n$ with $X_{i} = (X_{i1}, ..., X_{ip})$ in $\mathbb{R}^p$ and $Y_{i}\in\mathbb{R}$. Then a regression tree for predicting Y given $X = x$ is a model of the form.
$$f(x) = \sum_{m=1}^\mathbb{M} \mathbb{c}_{m}\mathbb{I}(x\in\mathbb{R_m})$$ 

#### Where $R_{1}, R_{2},...,R_{M}$ are partition regions of the form.


#### To create the decision tree of the given data set and try to predict best model
```{r}
library(rattle)
library(readr)
library(rpart)
heart.tree <- rpart(DEATH_EVENT ~ . , data = data)
fancyRpartPlot(heart.tree, sub = "")
plotcp(heart.tree)
bestcp <- heart.tree$cptable[which.min(heart.tree$cptable[,"xerror"]),"CP"]
bestcp
pruned.tree <- prune(heart.tree, cp = bestcp)
fancyRpartPlot(pruned.tree, sub = "")
```

#### I am going to create the conf.matrix to analysis the data.
```{r}
conf.matrix <- table(data$DEATH_EVENT, predict(pruned.tree,type="class"))
rownames(conf.matrix) <- paste("Actual", rownames(conf.matrix), sep = ":")
colnames(conf.matrix) <- paste("Pred", colnames(conf.matrix), sep = ":")
conf.matrix
```


#### Visualized the data to using ggplot and plot geom_point(), geom_vline(), and geom_hline() to relation between time, serum_sodium, and serum_creatinine. So, these variable are highly significant to DEATH_EVENT variable. 

```{r}
accuracy <- (conf.matrix[2,1] + conf.matrix[1,2])/sum(conf.matrix)
accuracy
```


```{r}
ggplot(data, aes(x = serum_sodium, y = time, color = DEATH_EVENT)) +
geom_point(position = "jitter") + 
geom_vline(xintercept = 137, lty = 2) +
geom_hline(yintercept = 60, lty = 2)
```



```{r}
ggplot(data, aes(x = serum_creatinine, y = time, color = DEATH_EVENT)) +
  geom_point(position = "jitter") + 
  geom_vline(xintercept = 1.6, lty = 2) +
  geom_hline(yintercept = 50, lty = 2)
```


# Analysis the data using the randomForest library.
```{r}
#install.packages("randomForest")
library(randomForest)
heart_forest <- randomForest(DEATH_EVENT ~ ., data = data, ntree = 500, mtry = 4)
varImpPlot(heart_forest)
```

```{r}
heart_complete <- dplyr::mutate_if(data, is.character, as.factor) 
conf.matrix <- table(heart_complete$DEATH_EVENT,predict(heart_forest, type = "class"))
rownames(conf.matrix) <- paste("Actual", rownames(conf.matrix), sep = ":")
colnames(conf.matrix) <- paste("Pred", colnames(conf.matrix), sep = ":")
conf.matrix
```

```{r}
plot(heart_forest)
```

# _Conclusion_
#### In this project, I am try to predict best multiple logistic regression model. First, I can use glm()function and try to fit the logistic regression in the family = binomial in my data set. Some of the variable are non significant and remove those variable with the help of forward, backword, and stepwise selection method, and try to keep AIC and BIC values are small as well as null deviance and residual deviance keep small. Also, data visualize the dafferent possible way such as box-plot, bar-plot, ggplot, and decision tree. However, my overall goal is to the help of data visualization to predict best logistic regression model. So, my best fit logistic regression model is:
### DEATH_EVENT = 12.498 + 0.045 * age - 0.068 * ejection_fraction - 0.082 * serum_sodium - 0.020 * time. 

# _Reference_

#### https://www.kaggle.com/andrewmvd/heart-failure-clinical-data?select=heart_failure_clinical_records_dataset.csv

## THE END